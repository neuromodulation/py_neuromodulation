{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Real-time feature estimation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementation of individual nm_streams\n\n*py_neuromodulation* was optimized for computation of real-time data streams.\nThere are however center -and lab specific hardware acquisition systems. Therefore, each experiment requires modules to interact with hardware platforms\nwhich periodically acquire data.\n\nGiven the raw data, data can be analyzed using *py_neuromodulation*. Preprocessing methods, such as re-referencing and normalization,\nfeature computation and decoding can be performed then in real-time.\n\nFor online as well as as offline analysis, the :class:`~nm_stream_abc` class needs to be instantiated.\nHere the `nm_settings` and `channels` are required to be defined.\nPreviously for the offline analysis, an offline :class:`~nm_generator` object was defined that periodically yielded data.\nFor online data, the :meth:`~stream_abc.run` function therefore needs to be overwritten, which first acquires data and then calls\nthe :meth:`~run_analysis.process` function.\n\nThe following illustrates in pseudo-code how such a stream could be initialized:\n\n```python\nfrom py_neuromodulation import nm_stream_abc\n\nclass MyStream(nm_stream_abc):\ndef __init__(self, settings, channels):\n    super().__init__(settings, channels)\n\ndef run(self):\n   features_ = []\n    while True:\n       data = self.acquire_data()\n       features_.append(self.run_analysis.process(data))\n       # potentially use machine learning model for decoding\n```\n## Computation time examples\n\nThe following example calculates for six channels, CAR re-referencing, z-score normalization and FFT features results the following computation time:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import py_neuromodulation as nm\nfrom py_neuromodulation import NMSettings\nimport numpy as np\nimport timeit\n\n\ndef get_fast_compute_settings():\n    settings = NMSettings.get_fast_compute()\n\n    settings.preprocessing = [\"re_referencing\", \"notch_filter\"]\n    settings.features.fft = True\n    settings.postprocessing.feature_normalization = True\n    return settings\n\n\ndata = np.random.random([1, 1000])\n\nprint(\"FFT Features, CAR re-referencing, z-score normalization\")\nprint()\nprint(\"Computation time for single ECoG channel: \")\nstream = nm.Stream(\n    sfreq=1000,\n    data=data,\n    sampling_rate_features_hz=10,\n    verbose=False,\n    settings=get_fast_compute_settings(),\n)\nprint(\n    f\"{np.round(timeit.timeit(lambda: stream.data_processor.process(data), number=100)/100, 3)} s\"\n)\n\nprint(\"Computation time for 6 ECoG channels: \")\ndata = np.random.random([6, 1000])\nstream = nm.Stream(\n    sfreq=1000,\n    data=data,\n    sampling_rate_features_hz=10,\n    verbose=False,\n    settings=get_fast_compute_settings(),\n)\nprint(\n    f\"{np.round(timeit.timeit(lambda: stream.data_processor.process(data), number=100)/100, 3)} s\"\n)\n\nprint(\n    \"\\nFFT Features & Temporal Waveform Shape & Hjorth & Bursts, CAR re-referencing, z-score normalization\"\n)\nprint(\"Computation time for single ECoG channel: \")\ndata = np.random.random([1, 1000])\nstream = nm.Stream(sfreq=1000, data=data, sampling_rate_features_hz=10, verbose=False)\nprint(\n    f\"{np.round(timeit.timeit(lambda: stream.data_processor.process(data), number=10)/10, 3)} s\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Those results show that the computation time for a typical pipeline (FFT, re-referencing, notch-filtering, feature normalization)\nis well below 10 ms, which is fast enough for real-time analysis with feature sampling rates below 100 Hz.\nComputation of more complex features could still result in feature sampling rates of more than 30 Hz.\n\n## Real-time movement decoding using the TMSi-SAGA amplifier\n\nIn the following example, we will show how we setup a real-time movement decoding experiment using the TMSi-SAGA amplifier.\nFirst, we relied on different software modules for data streaming and visualization.\n[LabStreamingLayer](https://labstreaminglayer.org) allows for real-time data streaming and synchronization across multiple devices.\nWe used [timeflux](https://timeflux.io) for real-time data visualization of features, decoded output.\nFor raw data visualization we used [Brain Streaming Layer](https://fcbg-hnp-meeg.github.io/bsl/dev/index.html).\n\nThe code for real-time movement decoding is added in the GitHub branch [realtime_decoding](https://github.com/neuromodulation/py_neuromodulation/tree/realtime_decoding).\nHere we relied on the [TMSI SAGA Python interface](https://gitlab.com/tmsi/tmsi-python-interface).\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}